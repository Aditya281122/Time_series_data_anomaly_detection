{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDA and Baselines\n",
        "\n",
        "This notebook covers data loading, STL Decomposition, and Changepoint Detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. STL Decomposition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'NAB\\\\data\\\\realKnownCause\\\\nyc_taxi.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 138\u001b[0m\n\u001b[0;32m    136\u001b[0m nab_root \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./NAB\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m file_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrealKnownCause/nyc_taxi.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 138\u001b[0m run_stl_pipeline(\n\u001b[0;32m    139\u001b[0m     nab_root\u001b[38;5;241m=\u001b[39mnab_root,\n\u001b[0;32m    140\u001b[0m     file_key\u001b[38;5;241m=\u001b[39mfile_key,\n\u001b[0;32m    141\u001b[0m     period\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m, \u001b[38;5;66;03m# Daily seasonality (30 min * 48 = 24h)\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/stl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    143\u001b[0m )\n",
            "Cell \u001b[1;32mIn[1], line 25\u001b[0m, in \u001b[0;36mrun_stl_pipeline\u001b[1;34m(nab_root, file_key, period, save_dir)\u001b[0m\n\u001b[0;32m     22\u001b[0m save_dir\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m df \u001b[38;5;241m=\u001b[39m load_series(nab_root, file_key)\n\u001b[0;32m     26\u001b[0m labels \u001b[38;5;241m=\u001b[39m load_labels(nab_root)\n\u001b[0;32m     27\u001b[0m label_times \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mget(file_key, labels\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file_key, []))\n",
            "File \u001b[1;32mc:\\Users\\adity\\OneDrive\\Documents\\anomaly\\src\\load_nab.py:24\u001b[0m, in \u001b[0;36mload_series\u001b[1;34m(nab_root, file_key)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     csv_path \u001b[38;5;241m=\u001b[39m root \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m file_key\n\u001b[1;32m---> 24\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_path)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# handle files without headers\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
            "File \u001b[1;32mc:\\Users\\adity\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[1;32mc:\\Users\\adity\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32mc:\\Users\\adity\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
            "File \u001b[1;32mc:\\Users\\adity\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[1;32mc:\\Users\\adity\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'NAB\\\\data\\\\realKnownCause\\\\nyc_taxi.csv'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Robustly determine project root and NAB path\n",
        "current_dir = Path(os.getcwd())\n",
        "if current_dir.name == 'notebooks':\n",
        "    project_root = current_dir.parent\n",
        "else:\n",
        "    project_root = current_dir\n",
        "\n",
        "# Add project root to sys.path\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.append(str(project_root))\n",
        "\n",
        "nab_root = str(project_root / 'NAB')\n",
        "print(f\"Project Root: {project_root}\")\n",
        "print(f\"NAB Root: {nab_root}\")\n",
        "# src/run_stl_analysis.py\n",
        "# Add project root to sys.path\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "from src.load_nab import load_series, load_labels, mark_anomaly_windows\n",
        "from src.kalman_model import detect_anomalies_by_residual\n",
        "from src.evaluate import compute_event_level_metrics\n",
        "\n",
        "def run_stl_pipeline(nab_root: str, file_key: str, period: int = 48, save_dir: str = \"./results/stl\"):\n",
        "    \"\"\"\n",
        "    Run STL Decomposition and detect anomalies on the residual.\n",
        "    \"\"\"\n",
        "    save_dir = Path(save_dir) / file_key.replace(\"/\", \"__\")\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # load data\n",
        "    df = load_series(nab_root, file_key)\n",
        "    labels = load_labels(nab_root)\n",
        "    label_times = labels.get(file_key, labels.get(\"data/\" + file_key, []))\n",
        "    df = mark_anomaly_windows(df, label_times, window_size=3) # gap=3 as requested\n",
        "    \n",
        "    values = df['value']\n",
        "    \n",
        "    print(f\"Running STL Decomposition (period={period})...\")\n",
        "    stl = STL(values, period=period, robust=True)\n",
        "    res = stl.fit()\n",
        "    \n",
        "    # Components\n",
        "    trend = res.trend\n",
        "    seasonal = res.seasonal\n",
        "    resid = res.resid\n",
        "    \n",
        "    # Plot Decomposition\n",
        "    fig, axes = plt.subplots(4, 1, figsize=(15, 12), sharex=True)\n",
        "    axes[0].plot(df['timestamp'], values, label='Original', color='black')\n",
        "    axes[0].set_title('Original Data')\n",
        "    axes[0].legend()\n",
        "    \n",
        "    axes[1].plot(df['timestamp'], trend, label='Trend', color='blue')\n",
        "    axes[1].set_title('Trend')\n",
        "    axes[1].legend()\n",
        "    \n",
        "    axes[2].plot(df['timestamp'], seasonal, label='Seasonal', color='green')\n",
        "    axes[2].set_title('Seasonal')\n",
        "    axes[2].legend()\n",
        "    \n",
        "    axes[3].plot(df['timestamp'], resid, label='Residual', color='red')\n",
        "    axes[3].set_title('Residual')\n",
        "    axes[3].legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_dir / \"stl_decomposition.png\")\n",
        "    print(f\"Saved decomposition plot to {save_dir / 'stl_decomposition.png'}\")\n",
        "    \n",
        "    # Detect Anomalies on Residuals\n",
        "    # We treat the residual as the \"anomaly signal\"\n",
        "    # We can use our robust thresholding logic here\n",
        "    \n",
        "    print(\"\\n--- Starting Threshold Sweep on Residuals (Rolling Sigma) ---\")\n",
        "    best_k = 3.0\n",
        "    best_f1 = -1.0\n",
        "    best_metrics = None\n",
        "    \n",
        "    # We use the ENTIRE residual series for detection (unsupervised)\n",
        "    # But for evaluation, we only care about the labeled region (usually full series for NAB)\n",
        "    \n",
        "    # Rolling Sigma on Residuals\n",
        "    # Window = 48 (Daily)\n",
        "    \n",
        "    for k_candidate in np.linspace(3.0, 12.0, 10):\n",
        "        # We can reuse detect_anomalies_by_residual\n",
        "        # actual = resid, mean = 0 (since it's residual)\n",
        "        # train_residuals = resid (we use the whole series history)\n",
        "        \n",
        "        flags_temp = detect_anomalies_by_residual(\n",
        "            resid.values, \n",
        "            np.zeros_like(resid.values), \n",
        "            resid.values, \n",
        "            k=k_candidate, \n",
        "            use_mad=False, \n",
        "            use_rolling=True,\n",
        "            window=48,\n",
        "            persistence=2\n",
        "        )\n",
        "        \n",
        "        m_evt = compute_event_level_metrics(df['is_anomaly'].values, flags_temp, gap=3)\n",
        "        f1 = m_evt['f1']\n",
        "        \n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_k = k_candidate\n",
        "            best_metrics = m_evt\n",
        "            \n",
        "    print(f\"--- Best Threshold: k={best_k:.1f} with F1={best_f1:.4f} ---\\n\")\n",
        "    \n",
        "    flags = detect_anomalies_by_residual(\n",
        "        resid.values, \n",
        "        np.zeros_like(resid.values), \n",
        "        resid.values, \n",
        "        k=best_k, \n",
        "        use_mad=False, \n",
        "        use_rolling=True,\n",
        "        window=48,\n",
        "        persistence=2\n",
        "    )\n",
        "    \n",
        "    # Save results\n",
        "    out_df = df.copy()\n",
        "    out_df['trend'] = trend\n",
        "    out_df['seasonal'] = seasonal\n",
        "    out_df['residual'] = resid\n",
        "    out_df['detected'] = flags\n",
        "    out_df.to_csv(save_dir / \"stl_results.csv\", index=False)\n",
        "    \n",
        "    metrics = {\n",
        "        \"event_level\": best_metrics,\n",
        "        \"best_k\": best_k\n",
        "    }\n",
        "    \n",
        "    with open(save_dir / \"metrics.json\", \"w\") as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "        \n",
        "    print(f\"Saved results to {save_dir}\")\n",
        "    print(\"STL Event Metrics:\", best_metrics)\n",
        "    return metrics\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # nab_root defined globally or dynamically above\n",
        "    nab_root = str(project_root / 'NAB')\n",
        "    file_key = \"realKnownCause/nyc_taxi.csv\"\n",
        "    run_stl_pipeline(\n",
        "        nab_root=nab_root,\n",
        "        file_key=file_key,\n",
        "        period=48, # Daily seasonality (30 min * 48 = 24h)\n",
        "        save_dir=\"./results/stl\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# --- Residual Diagnostics ---\n",
        "# We analyze the residuals from the STL decomposition to check for Normality and Heteroskedasticity.\n",
        "from src.plotting import plot_residual_diagnostics\n",
        "import pandas as pd\n",
        "\n",
        "# Load the STL results\n",
        "stl_results_path = Path(\"./results/stl/realKnownCause__nyc_taxi.csv/stl_results.csv\")\n",
        "if stl_results_path.exists():\n",
        "    df_stl = pd.read_csv(stl_results_path)\n",
        "    residuals = df_stl['residual'].values\n",
        "    \n",
        "    print(\"Generating Residual Diagnostics...\")\n",
        "    save_dir = Path(\"./results/eda\")\n",
        "    plot_residual_diagnostics(residuals, \"STL_Residuals\", save_dir)\n",
        "    \n",
        "    # Display the plots inline\n",
        "    from IPython.display import Image, display\n",
        "    display(Image(filename=save_dir / \"residual_hist.png\"))\n",
        "    display(Image(filename=save_dir / \"residual_qq.png\"))\n",
        "    display(Image(filename=save_dir / \"residual_rolling_std.png\"))\n",
        "else:\n",
        "    print(\"STL results not found. Run the STL cell above first.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Interpretation\n",
        "- **Seasonality**: The data shows strong daily and weekly seasonality, which STL captures well.\n",
        "- **Residuals**: The residuals are **heavy-tailed** (high Kurtosis) and **heteroskedastic** (variance changes over time), as seen in the Rolling Volatility plot.\n",
        "- **Implication**: Standard thresholding (mean +/- 3*std) will fail. We must use **Robust Statistics** (MAD) or **Adaptive Thresholding** (Rolling Sigma).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Bayesian Online Changepoint Detection (BOCPD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# src/run_bocpd_on_taxi.py\n",
        "import sys\n",
        "import os\n",
        "# Add project root to sys.path\n",
        "sys.path.append(os.path.abspath('..'))\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from src.bocpd_model import BOCPD\n",
        "from src.load_nab import load_series, load_labels, mark_anomaly_windows\n",
        "\n",
        "def run_bocpd_pipeline(nab_root: str, file_key: str, save_dir: str = \"./results/bocpd\"):\n",
        "    \"\"\"\n",
        "    Run BOCPD on NYC Taxi data.\n",
        "    \"\"\"\n",
        "    save_dir = Path(save_dir) / file_key.replace(\"/\", \"__\")\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # load data\n",
        "    df = load_series(nab_root, file_key)\n",
        "    labels = load_labels(nab_root)\n",
        "    label_times = labels.get(file_key, labels.get(\"data/\" + file_key, []))\n",
        "    df = mark_anomaly_windows(df, label_times, window_size=1)\n",
        "    \n",
        "    values = df['value'].values\n",
        "    # Normalize data for BOCPD (important for priors)\n",
        "    mean_val = np.mean(values)\n",
        "    std_val = np.std(values)\n",
        "    values_norm = (values - mean_val) / std_val\n",
        "    \n",
        "    print(\"Initializing BOCPD...\")\n",
        "    # Hazard rate: 1/100 implies we expect a changepoint every 100 steps roughly\n",
        "    # Taxi data (30 min) -> 100 steps = 50 hours (2 days). Reasonable.\n",
        "    bocpd = BOCPD(hazard_rate=1/100, mean_prior=0, var_prior=1, var_data=1)\n",
        "    \n",
        "    print(f\"Processing {len(values)} points...\")\n",
        "    for i, x in enumerate(values_norm):\n",
        "        bocpd.update(x)\n",
        "        if (i+1) % 1000 == 0:\n",
        "            print(f\"Processed {i+1}/{len(values)}\")\n",
        "            \n",
        "    R_mat = bocpd.get_run_length_matrix()\n",
        "    \n",
        "    # Calculate MAP run length path (for visualization)\n",
        "    # Or just probability of CP (r=0)\n",
        "    # R_mat has shape (T+1, T+1) because it includes initial state.\n",
        "    # We want to align with data, so we skip the first row (initial prior).\n",
        "    cp_probs = R_mat[1:, 0]\n",
        "    \n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n",
        "    \n",
        "    # Plot Data\n",
        "    axes[0].plot(df['timestamp'], values, label='Data', color='black', alpha=0.7)\n",
        "    # Plot Labeled Anomalies\n",
        "    anom_times = df[df['is_anomaly'] == 1]['timestamp']\n",
        "    axes[0].scatter(anom_times, df[df['is_anomaly'] == 1]['value'], color='red', label='Labeled Anomaly')\n",
        "    axes[0].set_title(f\"Data & Labels: {file_key}\")\n",
        "    axes[0].legend()\n",
        "    \n",
        "    # Plot CP Probability\n",
        "    axes[1].plot(df['timestamp'], cp_probs, label='Changepoint Prob (r=0)', color='blue')\n",
        "    axes[1].set_title(\"Changepoint Probability\")\n",
        "    axes[1].set_ylim(0, 1.1)\n",
        "    \n",
        "    # Highlight high CP probability regions\n",
        "    high_cp_indices = np.where(cp_probs > 0.5)[0]\n",
        "    if len(high_cp_indices) > 0:\n",
        "        axes[1].scatter(df['timestamp'].iloc[high_cp_indices], cp_probs[high_cp_indices], color='orange', s=10)\n",
        "        \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_dir / \"bocpd_result.png\")\n",
        "    print(f\"Saved plot to {save_dir / 'bocpd_result.png'}\")\n",
        "    \n",
        "    # Save CP probabilities\n",
        "    out_df = df.copy()\n",
        "    out_df['cp_prob'] = cp_probs\n",
        "    out_df.to_csv(save_dir / \"cp_probs.csv\", index=False)\n",
        "    \n",
        "    return out_df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # nab_root defined globally or dynamically above\n",
        "    nab_root = str(project_root / 'NAB')\n",
        "    file_key = \"realKnownCause/nyc_taxi.csv\"\n",
        "    run_bocpd_pipeline(\n",
        "        nab_root=nab_root,\n",
        "        file_key=file_key,\n",
        "        save_dir=\"./results/bocpd\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- Additional EDA: ACF and PACF ---\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' is loaded from previous cells (e.g. STL cell)\n",
        "# We'll reload it just in case or use the last known df\n",
        "if 'df' in locals():\n",
        "    fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
        "    plot_acf(df['value'], lags=50, ax=ax[0])\n",
        "    plot_pacf(df['value'], lags=50, ax=ax[1])\n",
        "    plt.tight_layout()\n",
        "    save_dir = Path('./results/eda')\n",
        "    save_dir.mkdir(parents=True, exist_ok=True)\n",
        "    plt.savefig(save_dir / 'acf_pacf.png')\n",
        "    print(f'Saved plot to {save_dir / \"acf_pacf.png\"}')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Dataframe 'df' not found. Run previous cells first.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}